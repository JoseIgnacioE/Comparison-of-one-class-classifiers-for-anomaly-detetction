{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Assignment 5",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "4qqV7S5a1Qz_"
      },
      "source": [
        "pip install brminer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P0QrNt_sT4Hg"
      },
      "source": [
        "pip install scikit-posthocs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ypPiQNZ3TiGQ"
      },
      "source": [
        "pip install Orange"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O5utiLjFXlUN"
      },
      "source": [
        "pip install Orange3-Associate"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AdA-1G9Tcgn5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "51483caf-927c-41d2-9efb-8c7d0603dece"
      },
      "source": [
        "import numpy as np\n",
        "import math\n",
        "import os\n",
        "import random\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import sys\n",
        "import math\n",
        "import csv\n",
        "import zipfile\n",
        "import Orange\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Post hoc tests\n",
        "import scikit_posthocs as sp\n",
        "from scipy.stats import friedmanchisquare\n",
        "\n",
        "# Classifiers\n",
        "import brminer\n",
        "from sklearn.svm import OneClassSVM\n",
        "from sklearn.ensemble import IsolationForest\n",
        "from sklearn.mixture import GaussianMixture\n",
        "\n",
        "# Others\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Activation, Flatten\n",
        "from zipfile import ZipFile\n",
        "\n",
        "# Sklearn\n",
        "from sklearn import metrics\n",
        "from sklearn.metrics import classification_report, roc_auc_score\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# BRMiner and distance metrics\n",
        "from sklearn.base import BaseEstimator\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.metrics.pairwise import euclidean_distances, manhattan_distances, cosine_distances\n",
        "from sklearn.utils.validation import check_X_y, check_array, check_is_fitted"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/statsmodels/tools/_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
            "  import pandas.util.testing as tm\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7e7yefYSttfi"
      },
      "source": [
        "from google.colab import files\n",
        "\n",
        "uploaded = files.upload()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7pAg0rypYp1M"
      },
      "source": [
        "class BRM_dissimilarities(BaseEstimator):\n",
        "    def __init__(self, classifier_count=100, bootstrap_sample_percent=100, use_bootstrap_sample_count=False,\n",
        "                 bootstrap_sample_count=0, use_past_even_queue=False, max_event_count=3, alpha=0.5, \n",
        "                 user_threshold=95, dissimilarity_meassure = euclidean_distances):\n",
        "        self.classifier_count = classifier_count\n",
        "        self.bootstrap_sample_percent = bootstrap_sample_percent\n",
        "        self.use_bootstrap_sample_count = use_bootstrap_sample_count\n",
        "        self.bootstrap_sample_count = bootstrap_sample_count\n",
        "        self.use_past_even_queue = use_past_even_queue\n",
        "        self.max_event_count = max_event_count\n",
        "        self.alpha = alpha\n",
        "        self.user_threshold = user_threshold\n",
        "        self.dissimilarity_meassure = dissimilarity_meassure\n",
        "        \n",
        "    def _evaluate(self, current_similarity):\n",
        "        if (current_similarity < 0):\n",
        "            current_similarity = 0\n",
        "\n",
        "        if (self.use_past_even_queue == False):\n",
        "            return -1+2*current_similarity\n",
        "        \n",
        "        result_similarity = (self.alpha * self._similarity_sum / self.max_event_count + (1 - self.alpha) * current_similarity)\n",
        "        if (result_similarity < 0):\n",
        "            result_similarity = 0\n",
        "\n",
        "        self._similarity_sum += current_similarity\n",
        "\n",
        "        if (len(self._past_events) == self.max_event_count):\n",
        "            self._similarity_sum -= self._past_events.pop(0)\n",
        "\n",
        "        self._past_events.append(current_similarity)\n",
        "\n",
        "        if (self._similarity_sum < 0):\n",
        "            self._similarity_sum = 0\n",
        "\n",
        "        return -1+2*result_similarity\n",
        "\n",
        "    def score_samples(self, X):\n",
        "        X = np.array(X)\n",
        "        X_test = self._scaler.transform(X)\n",
        "\n",
        "        result = []\n",
        "        batch_size = 100\n",
        "        for i in range(min(len(X_test), batch_size), len(X_test) + batch_size, batch_size):\n",
        "            current_X_test = X_test[[j for j in range(max(0, i-batch_size), min(i, len(X_test)))]]\n",
        "\n",
        "            current_similarity = np.average([np.exp(-np.power(np.amin(self.dissimilarity_meassure(current_X_test, self._centers[i]), axis=1)/self._max_dissimilarity, 2)/(self._sd[i])) for i in range(len(self._centers))], axis=0)\n",
        "        \n",
        "            result = result + [j for j in list(map(self._evaluate, current_similarity))]\n",
        "\n",
        "        return result\n",
        "        \n",
        "\n",
        "    def predict(self, X):\n",
        "        if (len(X.shape) < 2):\n",
        "            raise ValueError('Reshape your data')\n",
        "\n",
        "        if (X.shape[1] != self.n_features_in_):\n",
        "            raise ValueError('Reshape your data')\n",
        "\n",
        "        if not self._is_threshold_Computed:            \n",
        "            x_pred_classif = self.score_samples(self._X_train)            \n",
        "            x_pred_classif.sort()\n",
        "            self._inner_threshold = x_pred_classif[(100-self.user_threshold)*len(x_pred_classif)//100]\n",
        "            self._is_threshold_Computed = True\n",
        "\n",
        "        y_pred_classif = self.score_samples(X)\n",
        "        return [-1 if s <= self._inner_threshold else 1 for s in y_pred_classif]\n",
        "        \n",
        "\n",
        "    def fit(self, X, y = None):\n",
        "        # Check that X and y have correct shape\n",
        "        if y is not None:\n",
        "            X_train, y_train = check_X_y(X, y)\n",
        "        else:\n",
        "             X_train = check_array(X)\n",
        "                \n",
        "        self._similarity_sum = 0\n",
        "        self._is_threshold_Computed = False\n",
        "\n",
        "        self.n_features_in_ = X_train.shape[1]\n",
        "\n",
        "        if self.n_features_in_ < 1:\n",
        "            raise ValueError('Unable to instantiate the train dataset - Empty vector')     \n",
        "        \n",
        "        self._scaler = MinMaxScaler()\n",
        "        X_train = pd.DataFrame(X_train)\n",
        "        X_train = pd.DataFrame(self._scaler.fit_transform(X_train[X_train.columns]), index=X_train.index, columns=X_train.columns)\n",
        "\n",
        "\n",
        "        self._max_dissimilarity = math.sqrt(self.n_features_in_)\n",
        "        self._sd = np.empty(0)\n",
        "        sampleSize = int(self.bootstrap_sample_count) if (self.use_bootstrap_sample_count) else int(0.01 * self.bootstrap_sample_percent * len(X_train));\n",
        "        self._centers = np.empty((0, sampleSize, self.n_features_in_))\n",
        "\n",
        "        list_instances = X_train.values.tolist()\n",
        "        for i in range(0, self.classifier_count):            \n",
        "            centers = random.choices(list_instances, k=sampleSize)\n",
        "            self._centers = np.insert(self._centers, i, centers, axis=0)\n",
        "            self._sd = np.insert(self._sd, i, 2*(np.mean(self.dissimilarity_meassure(centers, centers))/self._max_dissimilarity)**2)\n",
        "\n",
        "        return self"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iKj36C4lcwS8"
      },
      "source": [
        "# Function to split target from data \n",
        "def splitdataset(train, test): \n",
        "    cols = len(train.columns)-1\n",
        "    train = train.drop(train[train.iloc[:,cols] == ' positive'].index)\n",
        "    ohe = OneHotEncoder(sparse=True)\n",
        "    objInTrain = len(train)\n",
        "    \n",
        "\n",
        "    allData = pd.concat([train, test], ignore_index=True, sort =False, axis=0)\n",
        "    AllDataWihoutClass = allData.iloc[:, :-1]\n",
        "    AllDataWihoutClassOnlyNominals = AllDataWihoutClass.select_dtypes(include=['object'])\n",
        "    AllDataWihoutClassNoNominals = AllDataWihoutClass.select_dtypes(exclude=['object'])\n",
        "\n",
        "    encAllDataWihoutClassNominals = ohe.fit_transform(AllDataWihoutClassOnlyNominals)\n",
        "    encAllDataWihoutClassNominalsToPanda = pd.DataFrame(encAllDataWihoutClassNominals.toarray())\n",
        "    \n",
        "    if AllDataWihoutClassOnlyNominals.shape[1] > 0:\n",
        "      codAllDataAgain = pd.concat([encAllDataWihoutClassNominalsToPanda, AllDataWihoutClassNoNominals], ignore_index=True, sort =False, axis=1)\n",
        "    else:\n",
        "      codAllDataAgain = AllDataWihoutClass\n",
        "\n",
        "    # Seperating the target variable \n",
        "    X_train = codAllDataAgain[:objInTrain]\n",
        "    y_train = train.values[:, -1]\n",
        "\n",
        "    X_test = codAllDataAgain[objInTrain:]\n",
        "    y_test = test.values[:, -1]\n",
        "    \n",
        "    mm_scaler = MinMaxScaler()\n",
        "    X_train_minmax = pd.DataFrame(mm_scaler.fit_transform(X_train[X_train.columns]), index=X_train.index, columns=X_train.columns)\n",
        "    X_test_minmax = pd.DataFrame(mm_scaler.transform(X_test[X_test.columns]), index=X_test.index, columns=X_test.columns)\n",
        "    \n",
        "    std_scaler = StandardScaler()\n",
        "    X_train_std = pd.DataFrame(std_scaler.fit_transform(X_train[X_train.columns]), index=X_train.index, columns=X_train.columns)\n",
        "    X_test_std = pd.DataFrame(std_scaler.transform(X_test[X_test.columns]), index=X_test.index, columns=X_test.columns)\n",
        "    \n",
        "    #X_train_minmax_std = pd.DataFrame(std_scaler.fit_transform(X_train_minmax[X_train_minmax.columns]), index=X_train_minmax.index, columns=X_train_minmax.columns)\n",
        "    #X_test_minmax_std = pd.DataFrame(std_scaler.transform(X_test_minmax[X_test_minmax.columns]), index=X_test_minmax.index, columns=X_test_minmax.columns)\n",
        "    \n",
        "    return X_train, X_test, y_train, y_test, X_train_minmax, X_test_minmax, X_train_std, X_test_std#, X_train_minmax_std, X_test_minmax_std\n",
        "\n",
        "# Function to make predictions \n",
        "def prediction(X_test, clf_object):  \n",
        "    y_pred = clf_object.score_samples(X_test) \n",
        "    return y_pred \n",
        "\n",
        "def result_of_Class(y_test, y_pred, saveFile):       \n",
        "    np.savetxt(saveFile, y_pred, fmt='%.4f')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QiYKEtptrjPj"
      },
      "source": [
        "# convert Arff files into CSV considering the column names\n",
        "def convert_arff_to_csv(arff_file, file_name):\n",
        "    f = open(arff_file)\n",
        "    content = f.readlines()\n",
        "    data = False\n",
        "    header = \"\"\n",
        "    newContent = []\n",
        "    for line in content:\n",
        "        if not data:\n",
        "            if \"@attribute\" in line:\n",
        "                attri = line.split()\n",
        "                columnName = attri[attri.index(\"@attribute\")+1]\n",
        "                header = header + columnName + \",\"\n",
        "            elif \"@data\" in line:\n",
        "                data = True\n",
        "                header = header[:-1]\n",
        "                header += '\\n'\n",
        "                newContent.append(header)\n",
        "        else:\n",
        "            newContent.append(line)\n",
        "\n",
        "    with open(file_name+\".csv\", \"w\") as outFile:\n",
        "        outFile.writelines(newContent)\n",
        "\n",
        "\n",
        "# Give the paths of every file and folder in the ZIP file\n",
        "def read_zip_files(zip_file_name):\n",
        "    with ZipFile(zip_file_name, 'r') as zipObj:\n",
        "        # Get a list of all archived file names from the zip\n",
        "        listOfFileNames = zipObj.namelist()\n",
        "        zipObj.extractall()\n",
        "    return listOfFileNames\n",
        "\n",
        "\n",
        "# Extract the names of all the files given the path of every file in the ZIP\n",
        "def extract_file_names(list_Of_File_Names):\n",
        "  list_of_files = []\n",
        "  h = 0\n",
        "  for i in range(len(list_Of_File_Names)):\n",
        "      names = list_Of_File_Names[i].split(\"/\")\n",
        "      if len(names)>2 and names[2]!='':\n",
        "         list_of_files.append(names[2])\n",
        "         csv_file = convert_arff_to_csv(listOfFileNamesComplete[i], list_of_files[h]) \n",
        "         h += 1\n",
        "  return list_of_files\n",
        "\n",
        "\n",
        "# Function importing Dataset \n",
        "def importdata(trainFile, testFile): \n",
        "    train = pd.read_csv(trainFile + \".csv\") \n",
        "    test = pd.read_csv(testFile + \".csv\") \n",
        "    return train, test\n",
        "\n",
        "\n",
        "\n",
        "# Put all the x_tests into a variable\n",
        "def x_test_into_a_variable(test,train):\n",
        "    X_train, X_test, y_train, y_test, X_train_minmax, X_test_minmax, X_train_std, X_test_std = splitdataset(train, test)\n",
        "    x_tests = [[X_train, X_test], [X_train_minmax, X_test_minmax], [X_train_std, X_test_std]]\n",
        "    return y_train, y_test, x_tests\n",
        "\n",
        "\n",
        "# classify data \n",
        "def classify_data(classifier, auc_array, x_train, y_train, x_test, y_test):\n",
        "    classifier.fit(x_train, y_train)\n",
        "    y_pred = classifier.score_samples(x_test)\n",
        "    auc = roc_auc_score(y_test,  y_pred)\n",
        "    auc_array.append(1 - auc if auc < 0.5 else auc)\n",
        "    return auc_array\n",
        "\n",
        "\n",
        "# Obtain AUC Score according to classifier\n",
        "def classifier_auc(classifier, train, test, auc_array, normalize):\n",
        "    X_train, X_test, y_train, y_test, X_train_minmax, X_test_minmax, X_train_std, X_test_std = splitdataset(train, test)\n",
        "    if normalize == 'No' or normalize == 'MinMax' or normalize == 'Std':\n",
        "        if normalize == 'No':\n",
        "            auc_array = classify_data(classifier, auc_array, X_train, y_train, X_test, y_test)\n",
        "        if normalize == 'MinMax':\n",
        "            auc_array = classify_data(classifier, auc_array, X_train_minmax, y_train, X_test_minmax, y_test)\n",
        "        if normalize == 'Std':\n",
        "            auc_array = classify_data(classifier, auc_array, X_train_std, y_train, X_test_std, y_test)\n",
        "        return auc_array\n",
        "    else:\n",
        "      print(\"Error: No normalization was indicated\")\n",
        "\n",
        "\n",
        "# Obtain dataset with AUC results\n",
        "def obtain_dataframe_with_auc_results(normalize, list_of_files, dis_measure):\n",
        "    classifiers = [['BRM', BRM_dissimilarities(dissimilarity_meassure = dis_measure)], ['GMM', GaussianMixture()],['ISOF', IsolationForest()],['ocSVM', OneClassSVM()]]\n",
        "    auc_results = pd.DataFrame(columns = ['BRM', 'GMM','ISOF', 'ocSVM'])\n",
        "    for classifier_name, clsf in classifiers:\n",
        "        auc_arr = []\n",
        "        for i in range(0,len(list_of_files),2):\n",
        "            train, test = importdata(list_of_files[i], list_of_files[i+1])\n",
        "            auc_arr = classifier_auc(clsf, train, test, auc_arr, normalize)\n",
        "        auc_results[classifier_name] = auc_arr\n",
        "    return auc_results"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0PCtqOa0_PBi"
      },
      "source": [
        "# create boxplot    \n",
        "def box_plot_creation(list_of_datasets, datasets_names):\n",
        "    figure, [ax1, ax2, ax3] = plt.subplots(nrows=1, ncols=3, figsize=(20,6))\n",
        "    boxplot_data = [[list_of_datasets[0], ax1],[list_of_datasets[1], ax2],[list_of_datasets[2],ax3]]\n",
        "\n",
        "    box_plot1 = ax1.boxplot(list_of_datasets[0].T, vert=True, patch_artist=True)\n",
        "    box_plot2 = ax2.boxplot(list_of_datasets[1].T, vert=True, patch_artist=True)\n",
        "    box_plot3 = ax3.boxplot(list_of_datasets[2].T, vert=True, patch_artist=True)\n",
        "    bp = [box_plot1, box_plot2, box_plot3]\n",
        "\n",
        "    colors = ['tab:blue', 'tab:orange', 'tab:green','tab:red']\n",
        "    for bplot in bp:\n",
        "        for patch, color in zip(bplot['boxes'], colors):\n",
        "            patch.set_facecolor(color)\n",
        "            patch.set(linewidth=1.5)\n",
        "        for whisker in bplot['whiskers']:\n",
        "            whisker.set(color ='k',\n",
        "                        linewidth = 2,\n",
        "                        linestyle =\":\")\n",
        "        for median in bplot['medians']:\n",
        "            median.set(color ='k',\n",
        "                      linewidth = 1.5)\n",
        "\n",
        "    x_names = ['a)','b)','c)']\n",
        "    axs = [ax1, ax2, ax3]\n",
        "    for norm, ax, x_n in zip(datasets_names, axs, x_names):\n",
        "        ax.set_xticklabels(['BRM', 'GMM', 'ISOF', 'ocSVM'], fontsize=16)\n",
        "        ax.set_title(norm, fontsize = 16)\n",
        "        ax.get_xaxis().tick_bottom()\n",
        "        ax.get_yaxis().tick_left()\n",
        "        ax.set_xlabel(x_n, fontsize=20)\n",
        "\n",
        "\n",
        "# Make the Friedman Test\n",
        "def friedman_test(list_of_datasets):\n",
        "    results = []\n",
        "    for df in list_of_datasets:\n",
        "        results.append(friedmanchisquare(df['BRM'],df['ISOF'],df['ocSVM'],df['GMM']))\n",
        "    return results\n",
        "\n",
        "\n",
        "# Connover test\n",
        "def post_hoc_tests(dataframe):\n",
        "    longDf=pd.melt(dataframe,var_name='criteria',value_name='score')\n",
        "    dunn = sp.posthoc_dunn(longDf,val_col='score',group_col='criteria',p_adjust='bonferroni')\n",
        "    conn = sp.posthoc_conover(longDf, val_col='score', group_col='criteria')\n",
        "    return dunn, conn\n",
        "\n",
        "\n",
        "# Make the post hoc tests\n",
        "def plot_post_hoc_tests(list_of_datasets):\n",
        "    for df in list_of_datasets:\n",
        "        plt.figure()\n",
        "        dunn, conn = post_hoc_tests(df)\n",
        "        heatmap_args = {'linewidths': 1, 'linecolor': '0', 'clip_on': False, 'square': True, 'cbar_ax_bbox': [0.80, 0.35, 0.04, 0.3]}\n",
        "        display(dunn,conn)\n",
        "        sp.sign_plot(conn, **heatmap_args)\n",
        "\n",
        "\n",
        "# Critical Difference Diagram\n",
        "def cd_diagram(dataframes):\n",
        "  for df in dataframes:\n",
        "    plt.figure\n",
        "    dunn, conn = post_hoc_tests(df)\n",
        "    classifier_names = conn.iloc[0,:].index\n",
        "    cd = Orange.evaluation.compute_CD(conn.iloc[0,:], 95,alpha='0.05', test='bonferroni-dunn') #tested on 95 datasets \n",
        "    print('cd=',cd)\n",
        "    Orange.evaluation.graph_ranks(conn.iloc[0,:], classifier_names, cd=cd, width=5, textspace=1.5, cdmethod=0)\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Z35WWbMKpVb"
      },
      "source": [
        "def normalizing_datasets():\n",
        "    zip_file = 'Unsupervised_Anomaly_Detection.zip'\n",
        "\n",
        "    listOfFileNamesComplete = read_zip_files(zip_file)\n",
        "    list_of_files = extract_file_names(listOfFileNamesComplete)\n",
        "\n",
        "    data = obtain_dataframe_with_auc_results('No', list_of_files, euclidean_distances)\n",
        "    data_minmax = obtain_dataframe_with_auc_results('MinMax', list_of_files, euclidean_distances)\n",
        "    data_std = obtain_dataframe_with_auc_results('Std', list_of_files, euclidean_distances)\n",
        "\n",
        "    dataframes = [data, data_minmax,data_std]\n",
        "    datasets_names = ['Without Normalization', 'Min-Max Normalization', 'Std Normalizing']\n",
        "\n",
        "    box_plot_creation(dataframes, datasets_names)\n",
        "    fried = friedman_test(dataframes)\n",
        "    display(fried)\n",
        "    plot_post_hoc_tests(dataframes)\n",
        "    cd_diagram(dataframes)\n",
        "\n",
        "def different_dissimilarity_measures():\n",
        "    zip_file = 'Unsupervised_Anomaly_Detection.zip'\n",
        "\n",
        "    listOfFileNamesComplete = read_zip_files(zip_file)\n",
        "    list_of_files = extract_file_names(listOfFileNamesComplete)\n",
        "\n",
        "    data_euc = obtain_dataframe_with_auc_results('No', list_of_files, euclidean_distances)\n",
        "    data_man = obtain_dataframe_with_auc_results('No', list_of_files, manhattan_distances)\n",
        "    data_cos = obtain_dataframe_with_auc_results('No', list_of_files, cosine_distances)\n",
        "\n",
        "    dataframes = [data_euc, data_man, data_cos]\n",
        "    datasets_names = ['Euclidean Distance', 'Manhattan Distance', 'Cosine Distance']\n",
        "\n",
        "    box_plot_creation(dataframes, datasets_names)\n",
        "    fried = friedman_test(dataframes)\n",
        "    display(fried)\n",
        "    plot_post_hoc_tests(dataframes)\n",
        "    cd_diagram(dataframes)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c9Z9Qw8NTK77"
      },
      "source": [
        "normalizing_datasets()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VVrscMjto6SE"
      },
      "source": [
        "different_dissimilarity_measures()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xiD5Ddt1tFdE"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}